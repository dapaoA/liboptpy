\documentclass[twoside]{article}

%\usepackage{aistats2022}
% If your paper is accepted, change the options for the package
% aistats2022 as follows:
%

\usepackage{aistats2022}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb,bbm}
\usepackage{algorithm,algorithmic}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{bm}

%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
\special{papersize = 8.5in, 11in}
\setlength{\pdfpageheight}{11in}
\setlength{\pdfpagewidth}{8.5in}
% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem*{thm*}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}

\newcommand{\calC} {\mathcal{C}}
\newcommand{\bfP} {\mathbf{P}}
\newcommand{\kl}{\mathbf {KL}}
\newcommand{\kll}{\mathcal {KL}}
\newcommand{\OTmp}[1]{\hat{\mathbf{P}}_{#1}}
\newcommand{\prox}{\operatorname{prox}}
\newcommand{\proj}{\operatorname{Proj}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\argmin}{\operatorname{argmin}}
%\newcommand{\tranT}{\mathsf T}
\newcommand{\tranT}{T}
\newcommand{\R}{\mathbbm{R}}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\mat}[1]{\mathbf{#1}}
%\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\vec}[1]{\bm{#1}}

\usepackage{xcolor}    
\newcommand{\changeHK}[1]{\textcolor{red}{#1}}
\newcommand{\changeXS}[1]{\textcolor{blue}{#1}}
\newcommand{\note}[1]{\textcolor{magenta}{#1}}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Dynamic Screening for $\ell_2$-norm Penalized Unbalanced Optimal Transport Problem}

\aistatsauthor{ Xun Su \And Author 2 \And  Author 3 }

\aistatsaddress{Waseda University \And  Institution 2 \And Institution 3 } ]

%\input{Sec_abstract}
%\input{Sec_introduction}
%\input{Sec_background}
%\input{Sec_UOT}
%\input{Sec_experiments}
%\input{Sec_conclusion}

\begin{abstract}
\note{HK comment:  We change the story: (OT and UOT $\rightarrow$ Lasso-type UOT $\rightarrow$ Propose Screening (No literature) $\rightarrow$ Evaluation)}
The Safe Screening technique saves computational time by freezing the zero elements in the sparse solution of the Lasso problem. Recently, researchers have linked the UOT problem to the Lasso problem. In this paper, we apply the newest Dynamic Screening framework to the $\ell_2$-norm penalized Unbalanced Optimal Transport (UOT) problem. We first apply the Screening method to the UOT problem. We find out that the specific structure for the UOT problem allows it to get better screening results than the Lasso problem. We propose the new Dynamic Screening algorithm and demonstrate its extraordinary effectiveness and potential to benefit from the unique structure of the UOT problem, our algorithm substantially improves the screening efficiency compared to the standard Lasso Screening algorithm without significantly increasing the computational burden. We demonstrate the advantages of the algorithm through some experiments on the Gaussian distributions and the MNIST dataset.
\end{abstract}


\section{INTRODUCTION}
\label{sec:int}
Optimal \changeHK{transport} (OT) has a long history in mathematics and has recently become prevalent due to its extraordinary performances in machine learning and statistical learning fields for measuring distances between \changeHK{two probability measures}. It has outperformed traditional methods in many different areas such as domain adaptation \citep{7586038}, generative models \citep{arjovsky2017wasserstein}, graph machine learning \citep{Maretic_NIPS_2019} and natural language processing \citep{084adf2f555549c493e0331a00e4ecad}. The popularity of OT is attributed to the introduction of the Sinkhorn's algorithm \citep{10.2307/2040061} for the entropy\changeHK{-regularized} Kantorovich formulation problem \citep{NIPS2013_af21d0c9}, which alleviate\changeHK{s} the computational burden in large-scale problems. 

\changeHK{Addressing one limitation that the standard OT problem handles only {\it balanced} samples, the unbalanced optimal transport (UOT) has been proposed envisioning a wider range of applications with {\it unbalanced} samples \citep{Caffarelli_AM_2010,chizat2017scaling}. This application fields include, for example, computational biology \citep{SCHIEBINGER2019928}, machine learning \citep{Janati_AISTATS_2019} and deep learning \citep{Yang_ICLR_2019}. Mathematically, UOT replaces the equality constraints with penalty functions on the marginal distributions with a divergence including KL divergence \citep{Liero:2018wo}, and $\ell_1$-norm \citep{Caffarelli_AM_2010} and $\ell_2$-norm distances \citep{refId0}. The KL-penalized UOT with an entropy regularizer can be solved by the Sinkhorn algorithm \citep{UOTSinkhorn2020}. It is fast, scalable, and differentiable, but suffers from a larger error of KL divergence, instability \citep{DBLP:journals/corr/Schmitzer16}, and a lack of sparsity in solution compared with other regularizers \citep{Blondel_AISTATS_2018}. In contrast, $\ell_2$-norm regularized UOT not only indicates a lower error, but also brings a sparse solution. This attracted the attention of researchers, and many algorithms have been developed \citep{Blondel_AISTATS_2018, Nguyen_arXiv_2022}}.

\changeHK{
Despite of the success in the development of fast and efficient optimization algorithms for UOT, computational burden still remains as a bottleneck for the large-scale application. This paper tackles this issue from a different direction independent of optimization algorithms. Recently, \cite{Chapel_NeurIPS_2021} suggested a mutual connection between the UOT problem and the Lasso-like problem \citep{Tibshirani_JRSS_1996,Efron_AM_2004} and the non-negative matrix factorization problem \citep{Lee_NIPS_2000}. This motivates us to adapt {\it safe screening} \citep{ghaoui2010safe} in these fields to accelerate the computation of the UOT problem. In fact, the OT and UOT problems expect the solution to be sparse due to the effectiveness of their optimal transport cost, and thus shares the similar idea with the Lasso problem. 
%Hence, we believe that it indicates the potential effectiveness of applying screening techniques in the Lasso problem to the UOT problem. 
Safe screening in the Lasso problem is a promising technique to speed up the computation by exploiting the sparsity of the solution. It eliminates elements in the solution that are guaranteed to be zero without solving the optimization problem. However, this straightforward application is not trivial, and the standard Lasso screening method cannot be applied directly to the UOT problem because it has un-regularized elements \note{(what means?)}. 
%Fortunately, the UOT problem has a particular structure compared to the Lasso problem. 
Specifically, while the Lasso problem has a dense {\it constraint matrix}, that of the UOT problem is extremely sparse and has a unique transport matrix structure. This would benefit a design of specialized screening method and the outcome. 
%
}

\changeHK{
In this paper, we propose a new dynamic screening method designated to the UOT problem. To this end, we first derive a dual formulation of the vectorized Lasso-like UOT problem. Then, particularly addressing the structure of the constraint matrix, a dynamic screening method is derived, which includes a new projection method and a screening region construction method. These are suitable for the UOT problem and could largely improve the effects. Different from the screening method in the Lasso problem, our proposed methods are .......
}


\changeHK{
\textbf{Contributions.}: 
\begin{itemize}
\item To the best of our knowledge, this work is a first dynamic screening method designated to the UOT problem, which is based on the Lasso-like formulation of the UOT problem and its dual formulation. Our proposed method is independent of optimization algorithms.
\item We propose a projection method by leveraging the specific structure of the UOT problem, and it largely decreases the errors in the projection process. 
We also propose a flexible two hyper-plane method  for every primal element to construct screening regions.
\item Numerical evaluation reveals their effectiveness in terms of projection distances and screening ratios comparing several methods including state-of-the-art methods in the Lasso problem.
\end{itemize}
}

%\begin{itemize}
%\item We apply the Screening technique in the Lasso community to the UOT problem and show its effectiveness. systematically constructed and applied the newest Dynamic Screening framework to the UOT problem.
%\item We generalize the Lasso-like Dynamic screening framework to the UOT problem with a newly proposed projection method, which utilizes the specific structure of the UOT problem and largely decreased the error caused in the projection process.
%\item We propose a new and specific Dynamic Screening method with a flexible Two hyper-plane construction method for every primal element, it largely improved the screening effectiveness without more computational burden. This method indicates that the Lasso-like problem with a specific structure like the UOT has the potential to design a better screening algorithm than the original Lasso problem.
%\end{itemize}

The paper is organized as follows. {\bf Section \ref{sec:pre}} presents preliminary descriptions of optimal transport and unbalanced optimal transport. The screening methods in Lasso-like problems are explained by addressing a dynamic screening framework. In {\bf Section \ref{sec:pro}}, our proposed screening method for the UOT problem is detailed. {\bf Section \ref{sec:exp}} shows numerical experiments.

\section{PRELIMINARIES}
\label{sec:pre}

$\mathbb{R}^n$ denotes $n$-dimensional Euclidean space, and $\mathbb{R}^n_+$ denotes the set of vectors in which all elements are non-negative. $\mathbb{R}^{m \times n}$ represents the set of $m \times n$ matrices. Also, $\mathbb{R}^{m \times n}_+$ stands for the set of $m \times n$ matrices in which all elements are non-negative. We present vectors as bold lower-case letters $\vec{a},\vec{b},\vec{c},\dots$ and matrices as bold-face upper-case letters $\mat{A},\mat{B},\mat{C},\dots$. The $i$-th element of $\vec{a}$ and the element at the $(i,j)$ position of $\mat{A}$ are represented respectively as $a_i$ and $\mat{A}_{i,j}$. In addition, $\one_n \in \mathbb{R}^n$ is the $n$-dimensional vector in which all the elements are one. For $\vec{x}$ and $\vec{y}$ of the same size, $\langle \vec{x},\vec{y} \rangle = \vec{x}^T\vec{y}$ is the Euclidean dot-product between vectors. For two matrices of the same size $\mat{A}$ and $\mat{B}$, $\langle \mat{A},\mat{B}\rangle={\rm tr}(\mat{A}^T\mat{B})$ is the Frobenius dot-product. We use $\|\vec{a}\|_2$ and $\|\vec{a}\|_1$ to represent the $\ell_2$ norm and $\ell_1$ norm of $\vec{a}$, respectively. 
%For a vector $\vec{x}$, the $i$-th element of $\exp (\vec{x})$ and $\log (\vec{x})$ respectively represent $\exp (\vec{x}_i)$ and $\log (\vec{x}_i)$. $\mathrm{KL}(\vec{x},\vec{y})$ stands for the KL divergence between $\vec{x} \in \mathbb{R}_+^n$ and $\vec{y} \in \mathbb{R}_+^n$, which is defined as $\sum_i \vec{x}_i \log {(\vec{x}_i/\vec{y}_i)} - \vec{x}_i + \vec{y}_i$. 
$D_\phi$ is the Bregman divergence with the strictly convex and differentiable function $\phi$, i.e., $D_\phi(\vec{a},\vec{b})=\sum_{i} d_\phi(a_i, b_i)=\sum_i [\phi(a_i) - \phi(b_i) - \phi'(a_i)(a_i -b_i)]$. In addition, we suggests a vectorization for $\mat{A} \in \mathbb{R}^{m \times n}$ as a lowercase letters $\vec{a} \in \mathbb{R}^{mn}$ and $\vec{a}=\text{vec}(\mat{A})=[\mat{A}_{1,1}, \mat{A}_{1,2}, \cdots, \mat{A}_{m,n-1}, \mat{A}_{m,n}]$.
 


\subsection{Optimal Transport and Unbalanced Optimal Transport}
{\bf Optimal Transport (OT):} \changeHK{Given two discrete probability measures $\vec{a}\in \R^{m}$ and $\vec{b} \in \R^{n}$, the standard OT problem seeks a corresponding {\it transport matrix} $\mat{T} \in \R_{+}^{m \times n}$ that minimizes the total transport cost \citep{Kantorovich_1942}. This is a linear programming problem formulated as
\begin{eqnarray}
\label{Eq:Standard_OT}
\operatorname{OT}(\vec{a},\vec{b}) &:=& \min_{ \mat{T} \in \R_{+}^{m \times n}} \langle \mat{C}, \mat{T} \rangle \\
\text{subject\ to}&& \mat{T} \one_n= \vec{a}, \mat{T}^{T}\one_m = \vec{b}, \notag
\end{eqnarray}
where $\mat{C} \in \mathbbm{R_{+}}^{m \times n}$ is the {\it cost matrix}. The constraints in (\ref{Eq:Standard_OT}) are so-called {\it mass-conservation constraints} or {\it marginal constraints}, and assume $\|\vec{a}\|_1 = \|\vec{b}\|_1$. Thus, the solution $\hat{\vec{t}}$ does not exist when $\|\vec{a}\|_1 \neq \|\vec{b}\|_1$. The obtained OT matrix $\mat{T}^*$ brings powerful distances as $\mathcal{W}_p = \langle \mat{T}^*,\mat{C} \rangle^{\frac{1}{p}}$, which is known as the $p$-th order {\it Wasserstein distance} \citep{Villani_2008_OTBook}. 
}

As $\vec{t} = \text{vec}({\mat{T}}) \in \mathbbm{R}^{mn}$ and $\vec{c} = \text{vec}({\mat{C}}) \in \mathbbm{R}^{mn}$, we reformulate Eq.(\ref{Eq:Standard_OT}) in a vector format as \citep{Chapel_NeurIPS_2021}
\begin{eqnarray}
\label{Eq:Vector_OT}
\operatorname{OT}(\vec{a},\vec{b}) &:=& \min_{\vec{t} \in \R_{+}^{mn}} \vec{c}^{\tranT}\vec{t} \\
\text{subject\ to}&& \mat{N}\vec{t} = \vec{a}, \mat{M}\vec{t} = \vec{b}, \notag
\end{eqnarray}
where $\mat{N} \in \R^{m \times mn}$ and $\mat{M} \in \R^{n \times mn}$ are two matrices composed of ``0" and ``1". $\mat{N}$ and $\mat{M}$ in case of $m=n=3$ are given, respectively, as
\begin{equation*}
\begin{split}
\mat{N}&=\begin{pmatrix}
1&1&1& 0& 0& 0& 0& 0&0\\
0 & 0& 0&1&1&1& 0& 0&0\\
0 & 0& 0& 0& 0& 0&1&1&1\\
\end{pmatrix},\\
\mat{M}&=\begin{pmatrix}
 1& 0& 0&1& 0& 0&1& 0&0\\
 0&1& 0& 0&1& 0& 0&1&0\\
 0& 0&1& 0& 0&1& 0& 0&1\\
 \end{pmatrix}.
  \end{split}
 \end{equation*}

 \changeHK{
 Note that $\mat{N}$ and $\mat{M}$ both have a specific structure, where each column has only one single non-zero element and this is equal to $1$. 
 }
 
{\bf Unbalanced Optimal Transport (UOT):} \changeHK{The marginal constraints in (\ref{Eq:Standard_OT}) may exacerbate degradation of the performance of some applications where weights need not be strictly preserved. In contrast, the UOT problem {\it relaxes} them by replacing the equality constraints with penalty functions on the marginal distributions with a divergence. Formally,} defining $\vec{y} = [\vec{a}, \vec{b}]^{\tranT}$ and $\mat{X} = [\mat{M}^{\tranT},\mat{N}^{\tranT}]^{\tranT}$, the UOT problem can be formulated introducing a penalty function for the histograms as \citep{Chapel_NeurIPS_2021}
\begin{equation}
\label{eq:uot}
\operatorname{UOT}(\vec{a},\vec{b}) := \min_{\vec{t} \in \R_{+}^{mn}} \vec{c}^{\tranT}\vec{t} + D_\phi(\mat{X}\vec{t},\vec{y}).
\end{equation}
\changeHK{
It is worth mentioning that this function is convex due to the convexity of the Bregman divergence.
}

{\bf Relationship with Lasso-like problem:} 
The Lasso-like problem has a general formula:
%
\begin{eqnarray*}
f(\vec{t}) &:=& g(\vec{t}) + D_\phi(\mat{X} \vec{t},\vec{y}).
%, t\in \mathbbm{R}^{mn}.
\end{eqnarray*}
 \changeHK{
Substituting $g(\vec{t}) = \lambda \|\vec{t}\|_1~(\lambda > 0)$ and $D_\phi(\mat{X} \vec{t},\vec{y}) = \frac{1}{2}\|\mat{X} \vec{t}-\vec{y}\|_2^2$ into above reduces to the standard $\ell_2$-norm regularized Lasso problem. On the other hand, addressing that $\vec{c}$ and $\vec{t}$ in (\ref{eq:uot}) are nonnegative, the term $\vec{c}^T\vec{t}$ is represented as $\vec{c}^T\vec{t}=\sum_i c_i t_i = \sum_ic_i |t_i|$. Therefore, the UOT problem in (\ref{eq:uot}) is equivalent to a weighted $\ell_1$-norm regularized Lasso-like problem. It is, however, that $\mat{X} = [\mat{M}^{\tranT},\mat{N}^{\tranT}]^{\tranT}$ in (\ref{eq:uot}) substantially differs from that of the Lasso problem. More concretely, the former $\mat{X}$ has a specific structure and has only two non-zero elements equal to $1$ in each row whereas the latter $\mat{X}$ in Lasso problem is non-structured and dense \citep{Chapel_NeurIPS_2021}.
}


\subsection{Dynamic Screening Framework}
\changeHK{
Solutions of many large-scale optimization problems tend to be sparse, and a large amount of computation is wasted on updating the zero elements during the optimization process. Screening technique is a well-known technique in the SVM \citep{Ogawa_ICML_2013} and Lasso problems, where the $\ell_1$-norm regularizer leads to a sparse solution for the problem \citep{ghaoui2010safe}. It can pre-select solutions that must be zero theoretically and freeze them before optimization computation, thus saving optimization time. Many safe screening methods have been proposed in this past decade \citep{Liu_ICML_2014,Wang_JMLR_2015}, and recent dynamic screening methods efficiently drop variable elements, which include Dynamic Screening \citep{7128732}, Gap Safe screening \citep{JMLR:v18:16-577} and Dynamic Sasvi \citep{NEURIPS2021_7b5b23f4}.}

Hereinafter, we briefly elaborate on the framework proposed in \citep{NEURIPS2021_7b5b23f4} to introduce the whole dynamic screening technique for the Lasso-like problem:
\begin{equation}
\label{eq:lassolike}
\min_{\vec{t}} \left\{ f(\vec{t}) := g(\vec{t}) + h(\mat{X} \vec{t}) \right\}.
\end{equation}

The Fenchel-Rockafellar Duality yields the dual problem as presented below:
\begin{thm}[Fenchel-Rockafellar Duality {\citep{Rockafellar_Springer_1998}}] 
\label{Thm:FRD}
If $d$ and $g$ are proper convex functions on $\mathbbm{R}^{m+n}$ and $\mathbbm{R}^{mn}$. Then we have the following:
\begin{equation*}
\min_{\vec{t}} g(\vec{t}) + h(\mat{X}\vec{t}) = \max_{\vec{\vec{\theta}}} -h^*(-\vec{\theta})-g^*(\mat{X}^{\tranT}\vec{\theta}).
\end{equation*}
\end{thm}

Because the primal function \changeHK{$h$} is always convex, the dual function \changeHK{$h^*$} is concave. Assuming \changeHK{$h^*$} is an $L$-strongly concave problem, we can design an area for any feasible $\tilde{\vec{\theta}}$ by the strongly concave property:

\begin{thm}[$L$-strongly concave {\citep[Theorem 5]{NEURIPS2021_7b5b23f4}}]\label{circle}
Considering problem in Eq.(\ref{eq:lassolike}), if function $d$ and $g$ are both convex, for $\forall \ \tilde{\vec{\theta}} \in{R^{m+n}}$ and satisfied the constraints on the dual problem, we have the following area constructed by its L-strongly concave property:  
$$
\begin{aligned}
\mathcal{R}^{C}:=\vec{\theta} \in \left\{\frac{L}{2}\|\vec{\theta}-\tilde{\vec{\theta}}\|_2^2+h^*(-\tilde{\vec{\theta}}) \leq h^*(-\vec{\theta})\right\}.
\end{aligned}
$$
\end{thm}

We know that the optimal solution for the dual problem $\hat{\vec{\theta}}$ satisfied the inequality, so the set is not empty.


\section{PROPOSED UOT SCREENING}
\label{sec:pro}

\changeHK{This section proposes a dynamic screening method designated to the UOT problem. For this purpose, we first derive the dual formulation of the Lasso-like formulated UOT problem in Eq.(\ref{eq:uot}). Concrete proofs of lemma and theorems are provided in the supplementary file.}


\subsection{Dual formulation of UOT}

\changeXS{As for the UOT problem in Eq.(\ref{eq:uot}), function } 
 $h(\mat{X} \vec{t}) = \frac{1}{2}\|\mat{X} \vec{t}-\vec{y}\|_2^2$, \changeXS{and $g(\vec{\theta})=\lambda \vec{c}^{T}\vec{t} ( \forall i, \vec{t}_i>0)$}. We obtain the dual form \changeXS{by} {\bf Theorem \ref{Thm:FRD}}:
\begin{lem}[Dual form of UOT problem]
For UOT problem in Eq.(\ref{eq:uot}), we obtain the following dual problem:
\begin{equation}
\begin{split}
-h^*(-\vec{\theta}) - g^*(\mat{X}^{\tranT}\vec{\theta})& = -\frac{1}{2}\|\vec{\theta}\|_2^2-\vec{y}^{\tranT}\vec{\theta} \\
 \text{s.t.} \quad \quad \vec{x}_p^{\tranT}\vec{\theta} -\lambda \vec{c}_p &\leq 0, \forall p,
 \end{split}
 \label{eq:uotdual}
\end{equation}
where $\vec{x}_p $ corresponds to the $p$-th column of $\mat{X}$. 
\end{lem}
It is clear that the strongly concave coefficient $L$ for the dual function $d$ is 1. These inequations in Eq.(\ref{eq:uotdual}) make up a dual feasible area written as $\mathcal{R}^{D}$, and the optimal \changeXS{dual solution $\hat{\vec{\theta}} \in \mathcal{R}^{D}$ because of \ref{thm:circle}.}\\
\changeXS{The KKT condition of the UOT problem holds for the optimal primal and dual solutions $ \hat{\vec{t}}$ and $\hat{\vec{\theta}}$ that:}
\begin{thm}[KKT condition] For the dual optimal solution $\hat{\vec{\theta}}$, we have the following relationship:
 \begin{equation}
\begin{split}
\vec{x}_p^{\tranT}\hat{\vec{\theta}} -\lambda \vec{c}_p \left\{
\begin{aligned}
< 0 \quad& \Rightarrow \hat{\vec{t}}_p = 0\\
= 0 \quad& \Rightarrow \hat{\vec{t}}_p \geq 0.
\end{aligned}
\right.
 \end{split}
 \label{eq:kkt}
\end{equation}
\end{thm}

\subsection{Detailed Screening Method}

Eq.(\ref{eq:kkt}) indicates a potential method to screening the primal variable. Since we do not know the information of $\hat{\vec{t}}$ directly, \changeXS{ Screening techniques try to construct an area $\mathcal{R}^{S}$  as small as possible and }also containing the $\hat{\vec{t}}$, if
\begin{equation}
\max_{\vec{t} \in \mathcal{R}^S} \vec{x}_p^{\tranT}\vec{\theta} -\lambda \vec{c}_p < 0.
\end{equation}
then we assert that:
 \begin{equation}
 \vec{x}_p^{\tranT}\hat{\vec{\theta}} -\lambda \vec{c}_p < 0,
 \label{eq:kktineq}
\end{equation}
which means the corresponding $\hat{\vec{t}}_p = 0$, and can be screened out. Noteworthy point is that, for the UOT problem, $\vec{x}_p = [...,0,1,0,...,0,1,0,...,]^{\tranT}$ has only two nonzero elements, $p_1$ and $p_2$, that are equal to 1. Therefore, we can set $\vec{\theta} = [\vec{u}^{\tranT},\vec{v}^{\tranT}]^{\tranT}$ and $\vec{u}\in\R^{m}, \vec{v}\in\R^{n}$, assuming $p=(I,J)$ where $I = p \mid m, J = p \mod m$. Subsequently, we rewrite Eq.(\ref{eq:kktineq}) as 
%
 \begin{equation}
\vec{u}_{I} + \vec{v}_{J}-\lambda \vec{c}_p < 0.
\end{equation}

\changeXS{If we have $\tilde{\vec{\theta}} \in \mathcal{R}^{D}$, knowing from {\bf Theorem \ref{thm:circle}}, we can construct the area $\mathcal{R}^{C}$ for the UOT problem:
\begin{lem}[The Circle Constraint for the UOT problem {\citep[Theorem 8]{NEURIPS2021_7b5b23f4}}]
\label{lem:uotrc}
For the UOT problem in Eq.(\ref{eq:uot}), $\mathcal{R}^{C}$ in {\bf Theorem \ref{thm:circle}} is equal to a circle constraint:
\begin{equation}
\mathcal{R}^{C} =\{\vec\theta\|(\vec{\theta}-\tilde{\vec{\theta}})^{\tranT}(\vec{\theta}-\vec{y})\leq 0\}
\label{eq:uotcircle}
\end{equation}
The $\vec\theta$ and $\vec y$ are the endpoints of the diameter of this circle. and $\hat{ \vec\theta} \in \mathcal{R}^{C}$
\end{lem}
If we obtain $\tilde{\vec{\theta}}$ in $\mathcal{R}^{D}$, combining $\mathcal{R}^{D}$ with $\mathcal{R}^{C}$ in {\bf Lemma \ref{lem:uotrc}}, we can use $\mathcal{R}^{C}\cap\mathcal{R}^{D}$ as the area $R^{S}$ in Eq.(\ref{eq:kktineq}). However, maxmizing on the intersection of a hyper-ball and a polytope cannot be computed easily cause the $n+m$-polytope has $nm$ Facets, \note{(????)} which costs too much for a screening operation.\\
The most direct method for dealing with the problem is to relax the polytope into a hyper-plane. Considering the specific structure of the $\mat X$ in the UOT problem, Our proposed method relaxes the polytope into two planes by combining the $nm$ dual constraints with a positive weight $\vec t_i \geq 0$, If the constraint is relevent to the dual elements $I$ and $j$, which would influence the screening of $\vec t_p$, we add it in group $A$, and the rest constraints are added in group $B$.}

\begin{thm}[Two Plane Screening for UOT]
\label{Thm:AreaScreeningUOT}
For every single primal variable $t_p$, let $A_p = \{ i \| 0\leq i<nm, i\mid m = I \vee i\mod m = J\}$, and $B_p = \{ i \| 0\leq i<nm, i \notin A_p\}$. Then, we construct a specific area $\mathcal{R}^{S}_{IJ}$ as presented below:
 \begin{equation}
\begin{split} 
\mathcal{R}^S_{IJ} = \left\{\vec{\theta} \ \left|\!\left| \ 
\begin{aligned}
 &\sum_{l\in A_p}(\vec{\theta}^{\tranT}\vec{x}_{l}\vec{t}_l - \lambda \vec{c}_l \vec{t})\leq 0, \\
 &\sum_{l\in B_p}(\vec{\theta}^{\tranT}\vec{x}_{l}\vec{t}_l - \lambda \vec{c}_l \vec{t})\leq 0, \\
  &(\vec{\theta}-\tilde{\vec{\theta}})^{\tranT}(\vec{\theta}-\vec{y})\leq 0 \ (\changeXS{\bf Lemma  \ref{lem:uotrc}})
\end{aligned}
\right.\right.
\right\}.
\end{split}
\label{eq:divide}
\end{equation}
We have $\hat{\vec{t}} \in \mathcal{R}^{C}\cap\mathcal{R}^{D} \subset \mathcal{R}^{S}_{IJ}$ (A simple proof process might be required for the $\subset$ relationship in the Appendix).
\end{thm}

\note{How does $\mathcal{R}^{S}_{IJ}$ play in the following??? Is in Algorithm 1 only?}

\changeXS{Our two planes area $\mathcal{R}^S_{IJ}$ is smaller than the relaxation in \citep{NEURIPS2021_7b5b23f4}. Benefiting from the structure of $\vec x_p$. As $\vec{x}_p^{\tranT}\vec{\theta} = \vec{u}_{I} + \vec{v}_{J}$, these two dual elements are the optimization directions, and only $m+n$ constraints are directly connected with them, dividing the constraints in two groups by the whether exists $\vec{u}_{I}$, and $\vec{v}_{J}$ can alleviate the huge relaxation influence of other secondary dual variables.\\
Before we start to screening with the area we constructed by {\bf Lemma \ref{lem:uotrc}} and {\bf Theorem \ref{Thm:AreaScreeningUOT}}, we first have to find a dual variable $\tilde{\vec{\theta}}\in\mathcal{R}^{D}$. Although there exists a relationship between the primal variable and dual variable that $\vec{\theta} = \vec{y} - \mat{X}\vec{t}$, inevitably, we often get $\vec{\theta} \notin \mathcal{R}^{D}$. This requires us to project $\vec{\theta}$ onto $\mathcal{R}^{D}$. In the Standrad Lasso problem, the dual constraints in {Eq.(\ref{eq:uotdual})} is $\|\vec{x}_p \hat{\vec{\theta}}\|_1\leq \lambda$, $x_p$ is almost dense and irregular, one has to use a shrinking method to obtain a $\tilde{\vec{\theta}} \in \mathcal{R}^{D}$:
\changeHK{\cite{xxxxx}}: 
\begin{equation*}
\tilde{\vec{\theta}} = \frac{\vec\theta}{\max(1, \|\frac{\mat{X}^{\tranT}\vec\theta}{\lambda}\|_{\infty})}
\end{equation*}
}
%Unlike in the Lasso problem, 
\changeXS{Dynamic Screening get more accurate screeing outcome because the optimization algorithms gradually provide $\vec t^{k} \Rightarrow \hat{\vec t}$, and also the dual variable $\vec \theta^{k} \Rightarrow \hat{\vec \theta}$. If $\tilde{\theta}\Rightarrow \hat{\vec \theta}$, the area $\mathcal{R}^{C}\cap\mathcal{R}^{D}$ would get smaller and has a better screening outcome. However, The Standard Lasso projection pushes the $\vec{\theta}$ far away from the optimum $\hat{\vec{\theta}}$, it suffers from the influence of small $\vec c_p$  as $$\tilde{\vec{\theta}} = \frac{\vec\theta}{\max(1, \|\frac{\mat{X}^{\tranT}\vec\theta}{\lambda \vec{c}^{T}}\|_{\infty})}$$, it will degenerate when one of the costs is $\vec{c}_p = 0$, and disable the screening process. Therefore, noting that, fo the UOT problem, it only allows $\vec{t}_p \geq 0$, and the $\vec{x}_p$ only consists of two non-zero elements, we can adapt a better projection method. The following theorem states this:}
\begin{thm}[UOT shifting projection]
\label{Thm:UOT_ShiftProjection}
For any $\vec{\theta} = [{\vec{u}}^{\tranT},{\vec{v}}^{\tranT}]^{\tranT}$, we compute the projection $\tilde{\vec{\theta}} = [\tilde{\vec{u}}^{\tranT},\tilde{\vec{v}}^{\tranT}]^{\tranT} \in \mathcal{R}^{D}$ as
\begin{eqnarray}
\tilde{\vec{u}}_I &=& {\vec{u}}_I - \max_{0\leq j < n} \frac{{\vec{u}}_I +{\vec{v}}_j - \lambda\vec{c}_{p}}{2} \notag\\
& =& \frac{{\vec{u}}_I +\lambda\vec{c}_{p}}{2} - \frac{1}{2}\max_{0\leq j < n} {\vec{v}}_j\\
\tilde{\vec{v}}_J &=& {\vec{v}}_J - \max_{0 \leq i < m} \frac{{\vec{u}}_i +{\vec{v}}_J - \lambda\vec{c}_{p}}{2}\notag\\
& =& \frac{{\vec{v}}_J +\lambda\vec{c}_{p}}{2} - \frac{1}{2}\max_{0 \leq i < m} {\vec{u}}_j
 \label{eq:uotproj}
\end{eqnarray}
\end{thm}
	\begin{figure}[h]
	\begin{center}	
	\includegraphics[width = \linewidth]{pic/shifting}
	\caption{Shifting on a 2$\times$2 matrix}
	\end{center}	
	\end{figure}


Since we now obtain $\tilde{\vec{\theta}}$ in $\mathcal{R}^{D}$, we additionally consider $\mathcal{R}^{C}$ in {\bf Theorem \ref{thm:circle}} to satisfy $\hat{\vec{t}} \in \mathcal{R}^{C}\cap\mathcal{R}^{D}$. However, the intersection of a sphere and a polytope cannot be computed \note{(????)} in $O(knm)$, where $k$ is a constant. Therefore, we propose a relaxation method, denoted as {\it two plane screening}. This divides the constraints into two parts, then we maximize the intersection of two hyperplanes and a hyper-ball. The next theorem states this:
%



We divide the constraints into two groups $A_p$ and $B_p$ for every single $p$, this problem can be solved easily by the Lagrangian method in constant time, the computational process is in Appendix. A

	\begin{figure}[h]
	\begin{center}	
	\includegraphics[width = 0.7\linewidth]{pic/divide}
	\caption{Selection of group $A_{IJ}$(red) and $B_{IJ}$(grey)}
	\end{center}	
	\end{figure}

\subsection{Screening Algorithms}

 \begin{algorithm}
 \caption{UOT Dynamic Screening Algorithm}
 \begin{algorithmic}[h]
 \label{Alg:UOTDynamicScreening}
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
 \renewcommand{\algorithmicensure}{\textbf{Output:}}
 \REQUIRE $\vec{t}_0, S \in R^{n\times m}, S_{ij}=1, (i,j) = mi+j$
 \ENSURE $S$
 \STATE \text{Choose a solver for the problem.}
 \FOR {$k = 0 \text{ to } K$}
 \STATE $\text{Projection } \tilde{\vec{\theta}} = \operatorname{Proj}(\vec{t}^k)$ 
 \FOR {$i = 0 \text{ to } m$}
  \FOR {$j = 0 \text{ to } n$}
  \STATE $\mathcal{R}^{S} \leftarrow \mathcal{R}_{ij}^S{(\tilde{\vec{\theta}},\vec{t}^k)}$
   \STATE $S \leftarrow {S_{ij} = 0 \text{ if } \max_{\vec{\theta} \in \mathcal{R}^S} {x_{(i,j)}}^{\tranT}\vec{\theta} <\lambda c_{(i,j)} }$
 \ENDFOR
  \ENDFOR
 \FOR {$(i,j) \in \{(i,j)\|S_{ij}=0\}$}
  \STATE $\vec{t}^k_{(i,j)} \leftarrow 0$
  \ENDFOR
  \STATE $\vec{t}^{k+1} = \operatorname{update}(\vec{t}^k)$
 \ENDFOR
   \RETURN $\vec{t}^{K+1}, S $ 
 \end{algorithmic} 
 \end{algorithm}
 \note{(Insert equation numbers in appropriate lines in {\bf Algorithm \ref{Alg:UOTDynamicScreening)}}}

It should be emphasized that the proposed screening method is {\it independent} of the optimization solver that you choose. We give the specific algorithm for $L_2$ UOT problem to show the whole optimization process as in {\bf Algorithm \ref{Alg:UOTDynamicScreening}}. The $\operatorname{update}(\vec{t})$ operator in the algorithm indicates the updating process for $\vec{t}$ according to the adopted optimizer.\\

\subsection{Computational Cost Analysis}
\note{XXXXXX}

\section{EXPERIMENTS}
\label{sec:exp}
In this section, we show the efficacy of the proposed methods using toy Gaussian models and the MNIST dataset.
\subsection{Projection Method}
To prove the effectiveness of our projection method compared with the traditional projection method in the Lasso problem, we compared the projection distance and screening ratio with randomly generated Gaussian measures by two projection methods. We set the $\lambda = \frac{\|\mathbf{X}^{\tranT}y\|}{100}$ and test for 10 different pairs.We choose the FISTA for solving the $L_2$ penalized UOT problems. Our projection method has only moved the dual point by a very small order of magnitude. It ensures that the points are kept at a smaller distance from the optimal solution and cause better screening effects.
	\begin{figure}[h]
	\begin{center}	
	\includegraphics[width = \linewidth]{pic/projdis}
	\caption{Distance of different projection method}
	\end{center}	
	\end{figure}
	\begin{figure}[htbp]
	\begin{center}	
	\includegraphics[width = \linewidth]{pic/sparse_proj}
	\caption{Screening ratio of different projection method}
	\end{center}	
	\end{figure}

\subsection{Divide Method}
We compared the screening ratio with three different methods, including our Divide method, Dynamic Sasvi method, and Gap method. Every method would use our projection method to get a better outcome, which also makes sure the difference in performance is only in the construction of the feasible domain. 
	\begin{figure}[h]
	\begin{center}	
	\includegraphics[width = \linewidth]{pic/screening_divide_ratio_long}
	\caption{Screening ratio of dividing method}
	\end{center}	
	\end{figure}

\subsection{Best Divide Method}
We compared the screening ratio with three different methods, including our Divide method, the Dynamic Sasvi method, and a random divide method. 
	\begin{figure}[h]
	\begin{center}	
	\includegraphics[width = \linewidth]{pic/divide}
	\caption{Comparing of our separation method with random separation method}
	\end{center}	
	\end{figure}

\subsection{Speed up Ratio}

We choose the FISTA method, Newton method, and Language method to test the screening ratio. 
	\begin{figure}[h]
	\begin{center}	
	\includegraphics[width = \linewidth]{pic/divide}
	\caption{speed up ratio for different solver}
	\end{center}	
	\end{figure}



\section{CONCLUSION}
Our algorithm is great, we are going to apply the method onto Sinkhorn


\clearpage
\bibliographystyle{apalike}
\bibliography{ref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% SUPPLEMENT (OPTIONAL) %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\appendix

\thispagestyle{empty}


% For one-column format, uncomment the following:
\onecolumn \makesupplementtitle
% For two-column format, uncomment the following:
%\twocolumn[ \makesupplementtitle ]

\section{PROOFS}
\subsection{Proof of Theorem \ref{Thm:UOT_ShiftProjection}}
For any $p \in {0,1,...,nm -1}$ we assume that $p = (I,J)$, then we can compute that:
 \begin{equation}
\begin{split} 
\vec{x}_p^{\tranT}\tilde{\vec{\theta}} &= \tilde{\vec{u}}_{I} + \tilde{\vec{v}}_J \\
				    &= {\vec{u}}_{I} + {\vec{v}}_J - \max_{0\geq j\geq n} \frac{{\vec{u}}_I +{\vec{v}}_j - \lambda \vec{c}_{p}}{2} - \max_{0 \geq i \geq m} \frac{{\vec{u}}_i +{\vec{v}}_J - \lambda \vec{c}_{p}}{2}\\
				    &= \frac{{\vec{u}}_{I} + {\vec{v}}_J}{2} - \max_{0\geq j\geq n} \frac{{\vec{v}}_j}{2} - \max_{0 \geq i \geq m} \frac{{\vec{u}}_i }{2} + \lambda \vec{c}_{p}\\
				    &= \frac{1}{2}\vec{x}_p^{\tranT}{\vec{\theta}} - \max_{0\geq j\geq n} \frac{{\vec{v}}_j}{2} - \max_{0 \geq i \geq m} \frac{{\vec{u}}_i }{2} +\lambda \vec{c}_{p} \\
				    &\leq \lambda \vec{c}_{p} 
 \end{split} 
\end{equation}
For $\forall p$, we have $\tilde{\vec{\theta}} \in \mathcal{R}^{D}$

\subsection{Proof of Theorem \ref{Thm:AreaScreeningUOT}}
We Generalize the problem as 
\begin{equation}
\max_{\vec{\theta} \in \mathcal{R}^{S}_{I}}{ \vec{\theta}_{I_1} +\vec{\theta}_{I_2} }
\end{equation}
Considering the center of the circle as $\vec{\theta}^o$, we define $\vec{\theta} = \vec{\theta}^{o} + q$, as ${ \vec{\theta}^{o}_{I_1} +\vec{\theta}^{o}_{I_2} }$ is a constant, the problem is equal to $\min_{\vec{\theta} \in \mathcal{R}^{S}_{I}}{- ( \vec{q}_{I_1} +\vec{q}_{I_2} )}$, we compute the Lagrangrian function of later:
\begin{equation}
\min_{\vec{q}} \max_{\eta,\mu,\nu \geq 0} L(\vec{q},\eta,\mu,\nu) =\min_{\vec{q}}\max_{\eta,\mu,\nu\geq0} - {\vec{q}_{I_1} - \vec{q}_{I_2} + \eta( \vec{q}^{\tranT}\vec{q} - r^2)+\mu( a^{\tranT}\vec{q} - e_a ) + \nu( b^{\tranT}\vec{q} - e_b )}
\end{equation}

 \begin{equation}
\begin{split} 
\frac{\partial L}{\partial \vec{q}_i} = \left\{
\begin{aligned}
-1 + 2\eta \vec{q}_i +\mu a_i + \nu b_i \quad& i = I_1, I_2\\
 2\eta \vec{q}_i +\mu a_i + \nu b_i \quad& i \neq I_1, I_2
\end{aligned}
\right.
 \end{split}
\label{eq:lang1}
\end{equation}

 \begin{equation}
\begin{split} 
{\vec{q}_i}^{*} = \left\{
\begin{aligned}
\frac{1- \mu a_i - \nu b_i}{2\eta} \quad& i = I_1, I_2\\
-\frac{\mu a_i + \nu b_i}{2\eta} \quad& i \neq I_1, I_2
\end{aligned}
\right.
 \end{split}
\label{eq:lang1}
\end{equation}

We can get the Lagrangian dual problem:
\begin{equation}
\max_{\eta,\mu,\nu\geq0} L(\eta,\mu,\nu) = \max_{\eta,\mu,\nu\geq0} \frac{\mu a_{I_1} + \nu b_{I_1}-1}{2\eta} +\frac{\mu a_{I_2} + \nu b_{I_2}-1}{2\eta}+ \eta({\vec{q}^{*}}^{\tranT}\vec{q}^{*}-r^2 )+\mu( a^{\tranT}\vec{q}^{*} - e_a ) + \nu( b^{\tranT}\vec{q}^{*} - e_b )
\end{equation}
From the KKT optimum condition, we know that if
\begin{equation}
\begin{split} 
 \eta ({\vec{q}^{*}}^{\tranT}\vec{q}^{*} -r^2) &= 0\\
 \mu( a^{\tranT}\vec{q}^{*} - e_a)&= 0\\
 \nu(b^{\tranT}\vec{q}^{*} - e_b) &=0
 \end{split}
\end{equation}
We set $\eta^{*}, \mu^{*}, \nu^{*}$ as the solution of the equations, which is also the solution of the dual problem. Firstly, we assume that $\eta^{*}, \mu^{*}, \nu^{*} \neq 0$, then the solution is equal to compute the following equations:

\begin{equation}
\begin{split} 
 & (1-\mu a_{I_1}-\nu b_{I_1})^2 + (1-\mu a_{I_2}-\nu b_{I_2})^2 + \sum^{m+n}_{i\neq I_1,I_2}(a_i\mu+b_i\nu)^2 - 4\eta^2 r^2 = 0 \\
 & a_{I_1}-\mu a_{I_1}^2-\nu b_{I_1}a_{I_1} + a_{I_2}-\mu a_{I_2}^2-\nu b_{I_2}a_{I_2} - \sum^{m}_{i\neq I_1,I_2}(a_i^2\mu +b_i a_i\nu) - 2\eta {e_a} = 0 \\
 & b_{I_1}-\nu b_{I_1}^2-\mu b_{I_1}a_{I_1} + b_{I_2}-\nu b_{I_2}^2-\mu b_{I_2}a_{I_2} - \sum^{m}_{i\neq I_1,I_2}(b_i^2\nu +b_i a_i\mu) - 2\eta {e_b} = 0 
 \end{split}
\end{equation}
Rearranged as:
\begin{equation}
\begin{split} 
 & 2-2\mu (a_{I_1}+a_{I_2})-2\nu(b_{I_1}+b_{I_2})+ \|a\|^2\mu^2+\|b\|^2\nu^2+2\mu\nu a^{\tranT}b - 4\eta^2 r^2 = 0 \\
 & (a_{I_1}+ a_{I_1}) - \|a\|^2\mu - a^{\tranT}b\nu - 2\eta {e_a} = 0 \\
 & (b_{I_1}+ b_{I_2}) - \|b\|^2\nu - a^{\tranT}b \mu - 2\eta {e_b} = 0 
 \end{split}
\end{equation}

we have 
\begin{equation}
\begin{split} 
\mu &= \frac{2( e_ba^{\tranT}b - e_a\|b\|^2 )\eta + (a_{I_1}+a_{I_2}) \|b\|^2 - (b_{I_1} + b_{I_2}) (a^{\tranT}b)}{ \|a\|^2 \|b\|^2 -a^{\tranT}b}\\
\nu &=\frac{2( e_aa^{\tranT}b - e_b\|a\|^2 )\eta + (b_{I_1}+b_{I_2}) \|a\|^2 - (a_{I_1} + a_{I_2}) (a^{\tranT}b)}{ \|a\|^2 \|b\|^2 -a^{\tranT}b}\\
 \end{split}
\end{equation}
set it as:
\begin{equation}
\begin{split} 
\mu &= s_1 \eta + s_2\\ 
\nu &= u_1 \eta + u_2
 \end{split}
 \label{eq:final}
\end{equation}

Then we can solve the $\eta$ as a quadratic equation:
\begin{equation}
\begin{split} 
0&=a\eta^2+b\eta+c\\
 a&= 4r^2 - s_1^2\|a\|^2 - u_1^2\|b\|^2 -2s_1 u_1a^{\tranT}b\\
b&=2(a_{I_1} + a_{I_2})s_1 +2(b_{I_1} + b_{I_2})u_1 - 2s_1s_2 \|a\|^2 - 2u_1u_2\|b\|^2 - 2(s_1u_2+s_2u_1)a^{\tranT}b \\
 c&=2(a_{I_1} + a_{I_2})s_2 +2(b_{I_1} + b_{I_2})u_2 -s_2^2\|a\|_1 -u_2^2\|b\|_1 - 2s_2u_2a^{\tranT}b -2
 \end{split}
\end{equation}

Then we can put it back into \ref{eq:final} and get $\mu, \nu$.

If the solution satisfied the constraints $\eta^{*}, \mu^{*}, \nu^{*} > 0$, then it is the solution.
However, if one of the dual variables is less than 0, the problem would degenerate into a simpler question. 

If only $\eta^{*}$ is larger than 0, 
 $\min_{\vec{\theta} \in \mathcal{R}^{S}_{I}}{- ( \vec{q}_{I_1} +\vec{q}_{I_2} )} = -\sqrt{2}r$

If only $\mu^{*}$ or $\nu^{*}$ is less than 0, we are optimizing on a sphere cap, the solution can be found in \cite[Appendix B]{NEURIPS2021_7b5b23f4}

if only $\eta^{*} \leq 0$:
As the sphere is inactivated, the problem gets maximum at every point of the intersection of two planes.
\begin{equation}
\min_{\vec{q}} \max_{\mu,\nu \geq 0} L(\vec{q},\mu,\nu) =\min_{\vec{q}}\max_{\mu,\nu\geq0} - {\vec{q}_{I_1} - \vec{q}_{I_2} +\mu( a^{\tranT}\vec{q} - e_a ) + \nu( b^{\tranT}\vec{q} - e_b )}
\end{equation}
To have a solution, the equations satisfied
 \begin{equation}
\begin{split} 
\frac{\partial L}{\partial q} = \left\{
\begin{aligned}
-1+\mu a_i + \nu b_i =0 \quad& i = I_1, I_2\\
-\mu a_i -\nu b_i \quad =0& i \neq I_1, I_2
\end{aligned}
\right.
 \end{split}
\end{equation}

As the equation satisfied, we can just set $\vec{q}_i^{*} = 0, i \neq I_1,I_2$, then we compute the  
 \begin{equation}
 \min_{\vec{\theta} \in \mathcal{R}^{S}_{I}}{- ( \vec{q}_{I_1} +\vec{q}_{I_2} )} = \frac{a_{I_2}e_b - b_{I_2}e_a -a_{I_1}e_b +b_{I_1}e_a}{a_{I_1}b_{I_2}-a_{I_2}b_{I_1}}
\end{equation}


\end{document}




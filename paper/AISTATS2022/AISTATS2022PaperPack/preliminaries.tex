\section{Preliminaries}
\label{sec:Preliminaries}
$\mathbb{R}^n$ denotes $n$-dimensional Euclidean space, and $\mathbb{R}^n_+$ denotes the set of vectors in which all elements are non-negative. $\mathbb{R}^{m \times n}$ represents the set of $m \times n$ matrices. Also, $\mathbb{R}^{m \times n}_+$ stands for the set of $m \times n$ matrices in which all elements are non-negative. $\Delta^n$ represents the probability simplex as $\Delta^n = \lbrace \vec{x} \in \mathbb{R}^n : \vec{x}_i \geq 0 , \sum_i \vec{x}_i = 1 \rbrace$. We present vectors as bold lower-case letters $\vec{a},\vec{b},\vec{c},\dots$ and matrices as bold-face upper-case letters $\mat{A},\mat{B},\mat{C},\dots$. The $i$-th element of $\vec{a}$ and the element at the $(i,j)$ position of $\mat{A}$ are represented respectively as $\vec{a}_i$ and $\mat{A}_{i,j}$. In addition, $\vec{1}_n \in \mathbb{R}^n$ is the $n$-dimensional vector in which all the elements are one. $\vec{\delta}_{x}$ is the Delta function at position $x$. For $\vec{x}$ and $\vec{y}$ of the same size, $\langle \vec{x},\vec{y} \rangle = \vec{x}^T\vec{y}$ is the Euclidean dot-product between vectors. For two matrices of the same size $\mat{A}$ and $\mat{B}$, $\langle \mat{A},\mat{B}\rangle={\rm tr}(\mat{A}^T\mat{B})$ is the Frobenius dot-product. For a vector $\vec{x}$, the $i$-th element of $\exp (\vec{x})$ and $\log (\vec{x})$ respectively represent $\exp (\vec{x}_i)$ and $\log (\vec{x}_i)$. $\mathrm{KL}(\vec{x},\vec{y})$ stands for the KL divergence between $\vec{x} \in \mathbb{R}_+^n$ and $\vec{y} \in \mathbb{R}_+^n$, which is defined as $\sum_i \vec{x}_i \log {(\vec{x}_i/\vec{y}_i)} - \vec{x}_i + \vec{y}_i$. $\mathrm{H}(\mat{T})$ represents the entropy term as $\mathrm{H}(\mat{T})=-\sum_{i,j}\mat{T}_{i,j}(\log \mat{T}_{ij}-1)$. $a = \mathcal{O}(f(n,\epsilon))$ expresses the {upper bound} satisfying the inequality $a \leq C \cdot f(n,\epsilon)$, where $C$ is independent of $n,\epsilon$. $a = \tilde{\mathcal{O}} (f(n,\epsilon))$ stands for the previous inequality for the constant $C$, which is dependent of $n,\epsilon$. Herein, $n$ and $\epsilon$ respectively represent a dimension and an approximation constant.

\subsection{Optimal transport}
The Kantorovich relaxation formulation of the optimal transport (OT) problem  is explained briefly. Let $\vec{a}$ and $\vec{b}$ be {probabilities} or positive weight vectors as $\vec{a}=(\vec{a}_1, \vec{a}_2, \ldots, \vec{a}_m)^T \in \mathbb{R}_+^m$ and $\vec{b}=(\vec{b}_1, \vec{b}_2, \ldots, \vec{b}_n)^T \in \mathbb{R}_+^n$, respectively. Given two empirical distributions, i.e., {discrete measures}, $\vec{\nu}=\!\sum_{i=1}^m a_i\vec{\delta}_{{x}_i}$, $\vec{\mu}=\!\sum_{j=1}^n b_j\vec{\delta}_{{y}_j}$ and the ground cost matrix $\mat{C} \in \mathbb{R}^{m \times n}$ between their supports, the problem can be formulated as
\begin{equation}
	\label{eq:FormulationOptimalTransport}
	 \defmin_{\scriptsize {\mat{T} \in \mathcal{U}(\vec{a},\vec{b})}}\ \langle \mat{C} ,\mat{T}\rangle,
\end{equation}
where $\mat{T} \in \mathbb{R}^{m \times n}$ represents the {\it transport matrix}, and where the domain $\mathcal{U}(\vec{a},\vec{b})$ is defined as 
\begin{equation}
	\label{eq:TransportPolytope}
	\mathcal{U}(\vec{a},\vec{b}) = \Bigl\{ \mat{T} \in \mathbb{R}^{m \times n}_{+}: \mat{T}\vec{1}_n = \vec{a},\mat{T}^{T}\vec{1}_m = \vec{b} \Bigr\},
\end{equation}
where $\mat{T}\vec{1}_n = \vec{a}$ and $\mat{T}^{T}\vec{1}_m = \vec{b}$ are the {\it marginal constraints}. Moreover, we present the sum of the two vectors respectively as $\alpha > 0$ and $\beta > 0$, i.e., $\sum_{i=1}^m {\vec{a}_i} = \alpha$ and $\sum_{i=1}^n {\vec{b}_i} = \beta$. Note that $\alpha$ is equal to $\beta$ in the standard OT formulation. The obtained OT matrix $\mat{T}^*$ brings powerful distances as $\mathcal{W}_p(\vec{\nu},\vec{\mu}) = \langle \mat{T}^*,\mat{C} \rangle^{\frac{1}{p}}$, which is known as the $p$-th order {\it Wasserstein distance} \cite{Villani_2008_OTBook}. It is used in various fields according to the value of $p$. Especially, the distance is applied to computer vision \cite{Levina_ICCV_2001} when $p=1$, and to clustering \cite{cuturi14barycenter} when $p=2$. Throughout this paper, when $\mat{C}$ is the {ground cost} matrix and $p=1$, we specifically designate the $1$-th order Wasserstein distance as the {\it OT distance}.
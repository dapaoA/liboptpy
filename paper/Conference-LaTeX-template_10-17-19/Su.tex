\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{dsfont}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{color}

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{definition}[thm]{Definition}
\newtheorem{assumption}[thm]{Assumption}

\newcommand{\R}{\mathbb{R}}
\newcommand{\one}{\mathds{1}}
\newcommand{\mat}[1]{\mathbf{#1}}
%\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\prox}{\operatorname{prox}}
\newcommand{\proj}{\operatorname{Proj}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\changeHK}[1]{\textcolor{red}{#1}}

\newcommand{\changeSX}[1]{\textcolor{blue}{#1}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Accelerating the Unbalanced Optimal Transport Problem Using Dynamic Penalty Updating
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Xun Su}
\IEEEauthorblockA{\textit{Graduate School of Fundamental Science and Engineering } \\
\textit{WASEDA University}\\
Tokyo, Japan \\
suxun$\_$opt@asagi.waseda.jp}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Hiroyuki Kasai}
\IEEEauthorblockA{\textit{School of Fundamental Science and Engineering} \\
\textit{WASEDA University}\\
Tokyo, Japan \\
hiroyuki.kasai@waseda.jp}

}

\maketitle

\begin{abstract}
With the increasing application of Optimal Transport (OT) in machine learning, the unbalanced optimal transport (UOT) problem, as a variant of optimal transport, has gained attention for its improved generality. There is an urgent need for fast algorithms that can efficiently handle large penalty parameters. In this paper, we prove that the recently proposed Majorize-Minimization algorithm for the UOT problem can be viewed as a form of Bregman proximal descent, and we propose to use dynamic penalty updating to make the algorithm converge quickly even for large penalties. By using a dynamic scheme, we can successfully compute better and sparser solutions for the large penalty parameter and approach the computational speed of the well-known Sinkhorn algorithm, which sacrifices accuracy by adding an entropy item.
\end{abstract}

\begin{IEEEkeywords}
Optimization, Optimal Transport
\end{IEEEkeywords}

\section{Introduction}
\label{sec:int}

Optimal transport (OT) has gained significant attention in the fields of machine learning and statistical learning due to its capacity to measure the distance between two probability measures. Combined OT methods have demonstrated superiority over traditional methods in areas such as domain adaptation \cite{Courty_PAMI_2017} and generative models \cite{arjovsky2017wasserstein}. Recently, OT theory has been applied to diverse technical fields, including graph analysis \cite{Huang_SigPro_2020,Huang_ICASSP_2021,Fang_AAAI_2023} and sequential data analysis \cite{Horie_EUSIPCO_2022}. The popularity of OT can be attributed to the introduction of Sinkhorn's algorithm \cite{Cuturi_NIPS_2013} for the entropy-regularized Kantorovich formulation problem, which has reduced the computational complexity associated with large-scale problems. However, the standard OT problem is limited to handling only {\it balanced} samples. To accommodate a wider range of applications with {\it unbalanced} samples, relaxed OT has been proposed, including partial OT (POT) \cite{ferradans2013regularized}, semi-relaxed OT (SROT) \cite{fukunaga_icassp2022,fukunaga_srsinkhorn}, and unbalanced optimal transport (UOT) \cite{Caffarelli_AM_2010,chizat2017scaling}. The UOT has been proposed as a method to replace equality constraints with KL divergence as a penalty function. It is solvable by adding an entropic regularization term and utilizing Sinkhorn's algorithm. Although it is fast, scalable, and differentiable, it is prone to instability and results in larger errors in solutions compared to other regularizers.

Chapel et al. recently proposed a Majorization-Maximization (MM) algorithm to solve the Unbalanced Optimal Transport (UOT) problem without adding an entropy term by exploiting the connection between UOT and non-negative matrix factorization \cite{Chapel_NeurIPS_2021}. Although their algorithm is GPU compatible and computationally efficient, it produces a solution that is blurrier than that of Sinkhorn's algorithm and is slower, especially for large penalty terms. In this paper, we propose a novel approach to speed up the optimization process by combining the MM algorithm with a dynamic penalty method that has been successfully used in Penalty and Augmented Lagrangian methods. Our approach is simple and effective, and significantly improves the computational speed of the MM algorithm for larger penalty terms.

Our contributions are twofold:
\begin{itemize}
\item We show that the MM algorithm for UOT can be derived from the Bregman Proximal Descent (BPD) algorithm with the theoratical best step size.
\item We propose a Dynamic Penalty MM algorithm (DPMM) that combines the MM algorithm with the dynamic penalty method to handle large penalization parameters. Our method achieves faster convergence and better performance, comparable to the Sinkhorn algorithm for balanced samples, and surpasses it for unbalanced samples.
\end{itemize}

\section{Preliminaries}
\subsection{Notation}
We use $\| \cdot \|_2$ to represent the Euclidean norm. $\mathbb{R}^n$ denotes $n$-dimensional Euclidean space, and $\mathbb{R}^n_+$ denotes the set of vectors in which all elements are non-negative. $\mathbb{R}^{n \times m}_+$ stands for the set of $n \times m$ matrices in which all elements are non-negative. We present vectors as bold lower-case letters $\vec{a},\vec{b},\vec{c},\dots$ and matrices as bold-face upper-case letters $\mat{A},\mat{B},\mat{C},\dots$. The $i$-th element of $\vec{a}$ and the element at the $(i,j)$ position of $\mat{A}$ are stated respectively as $a_i$ and ${A}_{i,j}$, the $i$-th column of $\mat{A}$ is represented as $\vec{a}_i$. In addition, $\one_n \in \mathbb{R}^n$ is the $n$-dimensional vector in which all elements are one. Additionally, we suggest vectorization for $\mat{A} \in \mathbb{R}^{n \times m}$ as lowercase letters $\vec{a} \in \mathbb{R}^{nm}$ and $\vec{a}=\text{vec}(\mat{A})=[{A}_{1,1}, {A}_{1,2}, \cdots, {A}_{m,n-1}, {A}_{m,n}]^T$, i.e., the concatenated vector of the transposed row vectors of $\mat{A}$.
%For $\vec{x}$ and $\vec{y}$ of the same size, $\langle \vec{x},\vec{y} \rangle = \vec{x}^T\vec{y}$ is the Euclidean dot-product between vectors.
For two matrices of the same size $\mat{A}$ and $\mat{B}$, $\langle \mat{A},\mat{B}\rangle={\rm tr}(\mat{A}^T\mat{B})$ is the Frobenius dot-product.

\subsection{Backgrounds on Optimal Transport}
The balanced OT problem is defined as
\begin{eqnarray}
\label{Eq:Standard_OT}
\operatorname{OT}(\vec{a},\vec{b}) &:=& \min_{ \mat{T} \in \R_{+}^{n \times m}} \langle \mat{C}, \mat{T} \rangle \\
\text{subject\ to}&& \mat{T} \one_n= \vec{a}, \mat{T}^{T}\one_m = \vec{b}. \notag
\end{eqnarray}
By relaxing the constraints using Kullback-Leibler (KL) divergence, we can obtain the UOT problem:
\begin{align}
\label{eq:uot}
&\operatorname{UOT}(\vec{a},\vec{b}) := \notag\\
&\min_{\mat{T} \in \R_{+}^{n \times m}} \langle \mat{C}, \mat{T} \rangle+ \tau \mathrm{KL}(\mat{T} \one_n,\vec{a}) + \tau \mathrm{KL}(\mat{T}^{T} \one_n,\vec{b}),
\end{align}
where $\mathrm{KL}(\vec{x},\vec{y})$ stands for the KL divergence between $\vec{x} \in \mathbb{R}_+^n$ and $\vec{y} \in \mathbb{R}_+^n$, which is defined as $\sum_i \vec{x}_i \log {(\vec{x}_i/\vec{y}_i)} - \vec{x}_i + \vec{y}_i$.


\subsection{MM Algorithm for UOT problem}
For this UOT problem, Chapel et al. consider it as a composite optimization problem \cite{Chapel_NeurIPS_2021}, which can be written as:
\begin{align}
\label{eq:reg}
\min_{\vec t} f(\vec t) = \min_{\vec t} g(\vec t) + h(\vec t),
\end{align}
where $g(\vec t) = \vec c^{T}\vec t$ and $h(\vec t) = \tau D_{\phi}(\mat H \vec t, \vec y)$. $\vec y = [\vec a^{T}, \vec b^{T}]^{T}$, $\mat H = [\mat {M}^{T}, \mat {N}^{T}]^{T}$, $\mat {M} $ and $\mat {N}$ are the indicate matrix to computing the sum of $\vec t$ according to rows and columns in $\mat T$ form.
where they propose the MM algorithm to solve the UOT problem, by building an auxiliary function for divergence $D_{\phi}$, assuming $\tilde{Z}_{i, j}=\frac{H_{i, j} \tilde{t}_j}{\sum_l H_{i, l} \tilde{t}_l}$

\begin{equation}
\begin{aligned}
\label{eq:af}
G_\tau(\boldsymbol{t}, \tilde{\boldsymbol{t}})&=\sum_{i, j} \tilde{Z}_{i, j} \phi\left(\frac{H_{i, j} t_j}{\tilde{Z}_{i, j}}\right)+\\
&\sum_j\left[\frac{c_j}{\tau}-\sum_i H_{i, j} \phi^{\prime}\left(y_i\right)\right] t_j+\\
&\sum_i\left[\phi^{\prime}\left(y_i\right) y_i-\phi\left(y_i\right)]\right .
\end{aligned}
\end{equation}
By minimizing $G_{\tau}( \vec t, \vec t^{k}) $, they can get the updating formula in the matrix form.
\begin{align}
\label{eq:update}
&\mat{T}^{(k+1)}=\notag\\
&\operatorname{diag}\left(\frac{\vec a}{\mat{T}^{(k)} \one_m}\right)^{\frac{1}{2}}\left(\mat{T}^{(k)} \odot \exp \left(-\frac{\mat C}{2 \tau}\right)\right)
\operatorname{diag}\left(\frac{\vec{b}}{\mat{T}^{(k) \top} \one_n}\right)^{\frac{1}{2}}.\notag\\
\end{align}

It is worth noting that the updating formula presented in (\ref{eq:update}) bears remarkable similarities with the widely popular Sinkhorn's algorithm, as it relies solely on matrix multiplication. While Sinkhorn's algorithm solves (\ref{eq:uot}) with an additional regularization term $\epsilon \mat H(\mat T) = \epsilon \langle \mat T,\ln(\mat T - 1)\rangle$ using an alternative matrix multiplication method, it shares a similar computational structure with MM algorithm. This feature also allows for the use of GPU acceleration to speed up the computation process.

\section{Proposed Algorithm}
\subsection{The MM algorithm and Its Bregman Proximal Descent Explanation}
Traditional Gradient descent can not be applied on some Banach Space which the dual space is not consistent with the primal one, Mirror descent\cite{doi:10.1137/1027074, BECK2003167} is a generalized method for handling related conditions. When it comes to the composite optimization problem, a proximal descent method can be combined with the mirror descent. Here, we would like to show that the Chapel's algorithm is one specific Bregman Proximal Descent \cite{DBLP:journals/coap/HanzelyRX21}.

%Assuming that the UOT problem can be represented as a composite function as
%\begin{align}
%\label{eq:comf}
%\operatorname{UOT}(\vec a, \vec b) = \min_{\mat T} f(\mat T)+ \psi_{\tau}(\mat T)
%\end{align}

\begin{thm}
Considering Applying BPD algorithm on {\ref{eq:reg}}, the updating formula can be written as
\begin{align}
\label{eq:update_md}
&\mat{T}^{(k+1)}=\notag\\
&\operatorname{diag}\left(\frac{\vec a}{\mat{T}^{(k)} \one_m}\right)^{\frac{\gamma}{\tau}}\left(\mat{T}^{(k)} \odot \exp \left(-\frac{\mat C}{2 \tau}\right)\right)
\operatorname{diag}\left(\frac{\vec{b}}{\mat{T}^{(k) \top} \one_n}\right)^{\frac{\gamma}{\tau}},\notag\\
\end{align}
where $\gamma$ is the step size in the BPD algorithm
\end{thm}
\begin{proof}
The proximal operator for the function $g$ is defined as:
\begin{align}
\prox_{\phi,\gamma}(x) = \argmin_{z}{(\frac{1}{\gamma}D_\phi(z,x)+g(z))}.
\end{align}
For the UOT problem, we can get:
\begin{align}
\prox_{\phi,\gamma}(\vec t) = \frac{\vec t}{e^{{\gamma \vec c}}},
\end{align}
then the BPD updating process is
\begin{align}
\label{eq:bpd}
\vec t^{k+1} = \prox_{\phi,\gamma}(\mat t^{k} - \gamma \nabla f(\vec t^{k})) = \frac{\vec t^{k}}{e^{\gamma ({\vec c + \nabla f(\vec t^{k})})}}.
\end{align}
We can obtain (\ref{eq:update_md}) by rearranging (\ref{eq:bpd}).
\end{proof}

It is oblivious that (\ref{eq:af}) is a special condition for the BPD algorithm with step size $\gamma = \frac{\tau}{2}$,
As proved in \cite{bauschke2017descent}, the theoretical step size should be $\frac{1}{L}$, and L is the relative smoothness constant for function f to function $\phi$.
\begin{definition}$[$Proposition 1.1 \cite{doi:10.1137/16M1099546}$]$
if function $h$ is $L$-relatively smooth to function $\phi$, $L\in {\mathbbm{R^{+}}}$, then function $h-L\phi$ is convex, or $D_h(\vec x,\vec y)<LD_{\phi}(\vec x,\vec y)$.
\end{definition}


\begin{thm}
For function $h(\vec t) = \tau D_{\phi}(\mat H \vec t, \vec y)$, its relatively smoothness $L = 2/\tau$
\end{thm}
\begin{proof}
If we want to proof $h-L\phi$ is convex, it is equal to proof for $\forall \vec d\in{\mathbbm{R}^n}$, we have $\vec d^{T}\Delta (h-L\phi) \vec d\succeq 0$
\begin{align*}
\vec d^{T}\Delta (\frac{h(\vec t)}{\tau})\vec d &= \sum_{i=1}^{n}{\frac{(\vec d^{T}\vec {m}_i)^2}{\vec t^{T}\vec M_i}}+\sum_{i=1}^{n}{\frac{(\vec d^{T}\vec n_i)^2}{\vec t^{T}\vec n_i}}\\
&\leq \sum_{i=1}^{n}\sum_{j=1}^{n^2}{\frac{(d_j M_{ij})^2}{ M_{ij}t_j}+\frac{(d_j N_{ij})^2}{ N_{ij}t_j}}\text{ (Cauchy Inequality)}\\
&=\sum_{j=1}^{n^2}(\sum_{i=1}^{n}(\frac{( M_{ij})^2}{ M_{ij}}+\frac{( N_{ij})^2}{ N_{ij}})\frac{d_{j}^{2}}{t_j})\\
&=\sum_{j=1}^{n^2}(\sum_{i=1}^{n}({ M_{ij}}+{ N_{ij}})\frac{d_{j}^{2}}{t_j})\\
&=\sum_{j=1}^{n^2}({\mat M^{T}\mathbbm{1}}+{\mat N^{T}\mathbbm{1}})_{j}\frac{d_{j}^{2}}{t_j}\\
&\leq \max_{j} ({\mat M^{T}\mathbbm{1}}+{\mat N^{T}\mathbbm{1}})_{j} \sum_{j}^{n^2}\frac{d_{j}^{2}}{t_j}\\
&\leq \frac{L}{\tau} \sum_{j}^{n^2}\frac{d_{j}^{2}}{t_j}\\
&= \frac{L}{\tau} \vec d^{T}\Delta h(\vec t)\vec d.
\end{align*}

Then we have $L/\tau \geq \max_{j} ({\mat M^{T}\mathbbm{1}}+{\mat N^{T}\mathbbm{1}})_{j} = 2$, and the best theoretical learning rate is 2/$\tau$. we can get the same updating formula as (\ref{eq:update}) after putting it into (\ref{eq:update_md})
\end{proof}

\subsection{Dynamic Penalized MM Algorithm}
\changeSX{The idea of relaxing a Constrained Optimization problem by penalty function is firstly proposed as the penalty function method or Barrier method. Similar ideas appeared in the Augmented lagrangian method to speed up the convergence of the lagrangian method. When the parameter of the penalty function is too small, it is difficult to obtain an accurate solution, while when it is too large, the function becomes ill-conditioned, leading to slow convergence. To avoid this, the penalty method \cite{349995, adaptive_p} and the augmented Lagrangian method \cite{doi:10.1137/1.9781611973365} often gradually increase the penalty parameter to prevent this situation.}

\changeSX{Since the UOT problem introduces a penalty function to relax the constraints, it also faces similar difficulties, especially when the parameter of the penalty function is very large, the ill-conditioned nature of the problem makes convergence difficult, as illustrated in Figure~\ref{Fig:ex2}. Therefore, we attempt to introduce the dynamic parameter update method of the penalty function and augmented Lagrangian method into the MM algorithm of the UOT problem and propose the DPMM algorithm.}

By using a {\it dynamic penalization term}, we gradually increase its influence throughout the optimization process. we demonstrate its effectiveness in our proposed algorithm.

Our proposed algorithm is summarized in {\bf Algorithm.~\ref{Alg1}}, where we set a small constant $q \in \R_+ $ and gradually increase the value of $\tilde{\tau}\in \R_+$ as the optimization error reduces. This warm start initialization allows the solver to avoid the ill-conditioned Hessian matrix issue encountered in the early stages, and the process enables our algorithm to obtain a sparser initialization,

\begin{algorithm}[t]
\caption{Dynamic Penalization MM algorithm (DPMM)}
\begin{algorithmic}[t]
\label{Alg1}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE $\mat{T}^0, \mat C, \tilde{\tau}, \tau, q$
\ENSURE $\mat T^{K}$
\STATE $\mat G = \operatorname{exp}(-\frac{\mat{C}}{2\tilde{\tau}})$
\FOR {$k = 1 \text{ to } K$}

\STATE $\vec{u} = (\frac{\vec{a}}{\mat T \one_n})^{\frac{1}{2}}, \vec v=(\frac{\vec{b}}{\mat{T}^{T} \one_m})^{\frac{1}{2}}$
\STATE $\mat{T}^{k} = \mat T^{k} \odot ( \vec{u}^{T} \mat G \vec{v})$
\STATE $err = \|\mat T^{k-1} - \mat T^{k}\|_2$
\IF {$err \leq \frac{q}{\tilde{\tau}} \text{ and } \tilde{\tau} \leq \tau$}
\STATE $\tilde{\tau} = \min(\tau, 2\tilde{\tau})$
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}
Since $\tilde{\tau}$ only doubles during the MM-IP algorithm for $O(log(\tau))$ times, the computation burden for recomputing matrix $\mat K = \exp \left(-\frac{\mat C}{2 \lambda}\right)$ is ignorable compared with the MM algorithm.

\begin{figure}[t]
\centering
\includegraphics[width = 0.99\linewidth]{pic/ex1}
\centering
\includegraphics[width = 0.99\linewidth]{pic/ex3}
\caption{Comparison of the convergence speed for different algorithms. The upper plot represents the results for balanced samples, while the lower plot displays the results for unbalanced samples. Using $\operatorname{OT}(\mat T^{*})$ to represents the value of {(\ref{Eq:Standard_OT})}, and $\operatorname{UOT}(\mat T^{k})$ to represents the function value calculated by replacing the optimal $\mat T$ in {(\ref{eq:uot})} with $\mat T^{k}$ }
\label{Fig:ex1}
\end{figure}



\section{Experiments}

We conducted experiments using randomly generated Gaussian distributions. In particular, we generated five pairs of 100-dimensional Gaussian distributions, each with the same mass. To test the performance of our approach in the case of unequal mass, we multiplied the mass of $\vec a$ by 1.2. For the mass-equal case, we obtained the analytical optimal solution $\mat T^{*}$ using linear programming. For both cases, we set $\tau = 1000$, and for Sinkhorn's algorithm, we set the regularizer parameter $\epsilon = 10^{-3}$. Additionally, we set the initial value of $\tilde{\tau} = 0.1$ and $q = 10^{-4}$ for our DPMM algorithm. We also incorporated Nesterov acceleration into our algorithm to obtain DPAMM. Figure~\ref{Fig:ex1} presents the results of our experiments.


\begin{figure}[tp]
\centering
\includegraphics[width = 0.99\linewidth]{pic/ex2}
\centering
\includegraphics[width = 0.99\linewidth]{pic/ex4}
\setlength{\belowcaptionskip}{-30pt}
\caption{\changeSX{Comparison of the solutions obtained using different optimization methods over 1000 iterations, The upper plot represents the results for balanced samples, while the lower plot displays the results for unbalanced samples. The MM algorithm fails to converge quickly to a near-sparse solution in any condition. MM-IP and AMM-IP methods perform significantly better, producing solutions not only that have a similar structure to Sinkhorn's but also better accuracy for unbalanced samples.}}
\label{Fig:ex2}
\end{figure}

These findings indicate that the MM algorithm struggles to minimize the transport cost when faced with a large penalization parameter. This leads to a significantly higher error compared to other methods in balanced samples. In the case of unbalanced samples, the algorithm's convergence is extremely slow. This is clearly illustrated in Figure~\ref{Fig:ex2}, where the large value of $\tau$ causes the MM algorithm preserve to be dense, which is inferior to both Sinkhorn's algorithm and our proposed methods. Our approach can not only quickly solve with a clear structure similar to Sinkhorn's algorithm, but also maintain a small error for unbalanced samples, where Sinkhorn suffers from errors brought by the regularizer.



\section{Conclusion}
We focus on the recent progress of the UOT optimization algorithm, using the BPD algorithm to illustrate the MM algorithm in the UOT problem. Our experimental results illustrate the effectiveness of our proposed DPMM algorithm. Compared to the MM algorithm, our proposed method can effectively handle the challenge of larger $\tau$ values by utilizing an dynamic penalization process that avoids poor initialization. This results in a superior solution quality that is competitive with the widely-known Sinkhorn algorithm. In the future, we plan to incorporate our expertise in the field of ALM to further accelerate the MM algorithm.




\bibliographystyle{IEEEtran}
\bibliography{ref}
\color{red}

\end{document}

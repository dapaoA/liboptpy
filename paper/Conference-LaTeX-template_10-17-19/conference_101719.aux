\relax 
\citation{Courty_PAMI_2017}
\citation{arjovsky2017wasserstein}
\citation{Huang_SigPro_2020,Huang_ICASSP_2021,Fang_AAAI_2023}
\citation{Horie_EUSIPCO_2022}
\citation{Cuturi_NIPS_2013}
\citation{ferradans2013regularized}
\citation{fukunaga_icassp2022,fukunaga_srsinkhorn}
\citation{Caffarelli_AM_2010,chizat2017scaling}
\citation{Chapel_NeurIPS_2021}
\citation{pmlr-v115-xie20b}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\newlabel{sec:int}{{I}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Preliminaries}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Notation}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Backgrounds on Optimal Transport}{1}{}\protected@file@percent }
\newlabel{Eq:Standard_OT}{{1}{1}}
\citation{Chapel_NeurIPS_2021}
\citation{doi:10.1137/1027074,BECK2003167}
\citation{DBLP:journals/coap/HanzelyRX21}
\citation{bauschke2017descent}
\citation{doi:10.1137/16M1099546}
\newlabel{eq:uot}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Algorithm}{2}{}\protected@file@percent }
\newlabel{eq:reg}{{3}{2}}
\newlabel{eq:af}{{4}{2}}
\newlabel{eq:update}{{7}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}The MM algorithm and Its Bregman Proximal Descent Explanation}{2}{}\protected@file@percent }
\newlabel{eq:update_md}{{11}{2}}
\citation{doi:10.1137/1.9781611973365}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Inexact Penalty Method MM algorithm}}{3}{}\protected@file@percent }
\newlabel{Alg1}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Inexact Penalized MM algorithm}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiments}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of the convergence speed for different algorithms. \leavevmode {\color  {black}The upper plot represents the results for balanced samples, while the lower plot displays the results for unbalanced samples.} Using $\operatorname  {OT}(\mathbf  {T}^{*})$ to represents the value of {(1\hbox {})}, and $\operatorname  {UOT}(\mathbf  {T}^{k})$ to represents the function value calculated by replacing the optimal $\mathbf  {T}$ in {(2\hbox {})} with $\mathbf  {T}^{k}$ }}{3}{}\protected@file@percent }
\newlabel{Fig:ex1}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{3}{}\protected@file@percent }
\bibstyle{IEEEtran}
\bibdata{ref}
\bibcite{Courty_PAMI_2017}{1}
\bibcite{arjovsky2017wasserstein}{2}
\bibcite{Huang_SigPro_2020}{3}
\bibcite{Huang_ICASSP_2021}{4}
\bibcite{Fang_AAAI_2023}{5}
\bibcite{Horie_EUSIPCO_2022}{6}
\bibcite{Cuturi_NIPS_2013}{7}
\bibcite{ferradans2013regularized}{8}
\bibcite{fukunaga_icassp2022}{9}
\bibcite{fukunaga_srsinkhorn}{10}
\bibcite{Caffarelli_AM_2010}{11}
\bibcite{chizat2017scaling}{12}
\bibcite{Chapel_NeurIPS_2021}{13}
\bibcite{pmlr-v115-xie20b}{14}
\bibcite{bauschke2017descent}{15}
\bibcite{doi:10.1137/16M1099546}{16}
\bibcite{doi:10.1137/1.9781611973365}{17}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \leavevmode {\color  {black}Comparison of the solutions obtained using different optimization methods over 1000 iterations, The upper plot represents the results for balanced samples, while the lower plot displays the results for unbalanced samples. The MM algorithm fails to converge quickly to a near-sparse solution in any condition. MM-IP and AMM-IP methods perform significantly better, producing solutions not only that have a similar structure to Sinkhorn's but also better accuracy for unbalanced samples.}}}{4}{}\protected@file@percent }
\newlabel{Fig:ex2}{{2}{4}}
\@writefile{toc}{\contentsline {section}{References}{4}{}\protected@file@percent }
\gdef \@abspage@last{4}
